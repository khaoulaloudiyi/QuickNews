{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Presse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaoulaloudiyi/QuickNews/blob/master/Presse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHdpHSWd_VcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "b962887a-f970-4e51-93f4-828e7faf6396"
      },
      "source": [
        "!pip install dateparser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dateparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/d5/5a2e51bc0058f66b54669735f739d27afc3eb453ab00520623c7ab168e22/dateparser-0.7.6-py2.py3-none-any.whl (362kB)\n",
            "\r\u001b[K     |█                               | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 2.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 3.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 40kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 61kB 3.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 81kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 92kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 266kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 276kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 286kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 296kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 307kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 317kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 327kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 337kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 348kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 358kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 368kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from dateparser) (2.8.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser) (1.5.1)\n",
            "Requirement already satisfied: regex!=2019.02.19 in /usr/local/lib/python3.6/dist-packages (from dateparser) (2019.12.20)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from dateparser) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->dateparser) (1.15.0)\n",
            "Installing collected packages: dateparser\n",
            "Successfully installed dateparser-0.7.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cuLXKie_f42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4cf71bae-17cb-485d-869b-c0b243f0623b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/3wH3TqjVVCuHIa70PukYjt5PGLe_JyNq5wXXKwc7M48RRNgBHxjoct0\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vun791UC_F-z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a16d314-5ab1-44dd-9d1c-16dcaa814292"
      },
      "source": [
        "cd /content/drive/My Drive//MediaInsights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MediaInsights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMJcyVGeINXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b52405d3-89e9-4a90-be13-c992281dc83b"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MediaInsights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwHQAk6z_AIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "35ce635b-dfdc-4edd-d9d0-42364d8984a3"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import csv\n",
        "import dateparser\n",
        "from textblob import TextBlob\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from collections import Counter \n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stop_words\n",
        "import spacy \n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import nltk \n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOx7d0MCEHNH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ec97b4e9-045e-47cd-91b0-0853b2dacf8e"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "swa = stopwords.words('french')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kolBwjDQ_AIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cette partie doit être executer une seule fois pour créer le header des deux fichiers csv\n",
        "\n",
        "#Ce fichier joue un rôle principal pour la mise à jour puisqu'il contient le champ lastDate qui réfère à la date ou la dernière mise à jour a été effectué et le champ maxid pour faciliter la recherche du plus grand Id \n",
        "with open('presse_lastUpdate.csv','w',encoding=\"utf-8\") as csv_file:\n",
        "    fieldnames=['country', 'Publication', 'maxId', 'lastDate']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    \n",
        "#Ce fichier est réservé pour stocker les différents articles récupéré grâce au scraping \n",
        "with open('Articles.csv','w',encoding=\"utf-8\") as csv_file:\n",
        "    fieldnames=['Id','Title','Author','Date','Publication','URL','Content','Language','Country', \"sentiment\", \"Summary\"]\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-ppz8Td_AI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#D'abord il faut comprendre la structure des sites web de la presse, chaque presse possède un archive où tous les articles sont stocké, sous forme d'une dizaine d'articles dans plusieurs pages, \n",
        "#chaque page de ce dernier contient plusieurs articles référencé par des liens, et tous ces articles possède le même format et contient l'ensemble des informations sauf \n",
        "#l'information sur le \"sentiment\"\n",
        "\n",
        "#Le principe du Scraping qu'on a utilisé est le suivant \"exemple avec Hespress Ar mais la même chose se répète avec les autres presses\":\n",
        "#d'abord on effectue une étude pour comprendre la structure d'une page de l'archive (code html) afin de récupérer les liens des articles seulement\n",
        "#apres on analyse la structure d'un de ces articles (html) , On parcour l'ensemble des pages de l'archive grâce à la fonction \"add_newArticles_hespres\" cette\n",
        "#fonction utilise la variable p qui représente le numéro de page de l'archive a scrapé, on scrape cette page puis on collecte les liens\n",
        "#href des articles et pour chaque lien on scrape l'article avec la fonction \"scrap_article_Hespress\" et on l'ajout à la dataset Articles\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmH-_8CkEc9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenizing sentences into words\n",
        "def tokenizer(s):\n",
        "    tokens = []\n",
        "    for word in s.split(' '):\n",
        "        tokens.append(word.strip().lower()) #strip remove spaces at the beginning and at the end of the string\n",
        "    return tokens\n",
        "\n",
        "#tokenizing document into sentences \n",
        "def sent_tokenizer(s):\n",
        "    sents = []\n",
        "    for sent in s.split('.'):\n",
        "        sents.append(sent.strip())\n",
        "    return sents\n",
        "\n",
        "def count_words(tokens):\n",
        "    word_counts = {}\n",
        "    for token in tokens:\n",
        "        if token not in stop_words and token not in STOP_WORDS and token not in swa and token not in punctuation:\n",
        "            if token not in word_counts.keys():\n",
        "                word_counts[token] = 1\n",
        "            else:\n",
        "                word_counts[token] += 1\n",
        "    return word_counts\n",
        "\n",
        "def word_freq_distribution(word_counts):\n",
        "    freq_dist = {}\n",
        "    max_freq = max(word_counts.values())\n",
        "    for word in word_counts.keys():  \n",
        "        freq_dist[word] = (word_counts[word]/max_freq)\n",
        "    return freq_dist\n",
        "\n",
        "def score_sentences(sents, freq_dist, max_len=40):\n",
        "    sent_scores = {}  \n",
        "    for sent in sents:\n",
        "        words = sent.split(' ')\n",
        "        for word in words:\n",
        "            if word.lower() in freq_dist.keys():\n",
        "                if len(words) < max_len:\n",
        "                    if sent not in sent_scores.keys():\n",
        "                        sent_scores[sent] = freq_dist[word.lower()]\n",
        "                    else:\n",
        "                        sent_scores[sent] += freq_dist[word.lower()]\n",
        "    return sent_scores\n",
        "\n",
        "def summarize(sent_scores, k):\n",
        "    top_sents = Counter(sent_scores) \n",
        "    summary = ''\n",
        "    scores = []\n",
        "    \n",
        "    top = top_sents.most_common(k)\n",
        "    for t in top: \n",
        "        summary += t[0].strip()+'. '\n",
        "        scores.append((t[1], t[0]))\n",
        "    return summary[:-1], scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD2Hnlrm_AI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#le matin\n",
        "def scrap_article_LeMatin(site, date_max, Id, writer):\n",
        "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "    req = Request(site,headers=hdr)\n",
        "    page = urlopen(req)\n",
        "    soup = BeautifulSoup(page)\n",
        "\n",
        "    title = soup.find(\"h1\", {\"id\": \"title\"})\n",
        "    title = title.text\n",
        "\n",
        "    time = soup.find(\"div\", {\"class\": \"col-lg-7 col-12 float-left text-left\"}).find('time').text\n",
        "    date_publication = dateparser.parse(time)\n",
        "    year = date_publication.strftime(\"%Y\")\n",
        "    month = date_publication.strftime(\"%m\")\n",
        "\n",
        "    content_a = soup.findAll(\"div\", {\"class\": \"card-body p-2\"}) \n",
        "    content = \" \"\n",
        "    for x in content_a:\n",
        "        for c in x.findAll('p'):\n",
        "            if c.text != \"\\n\":\n",
        "                content +=  c.text + \"\\n\"\n",
        "    \n",
        "    \n",
        "    author ='Le matin' \n",
        "    language = 'Fr'\n",
        "    country = 'Morocco'\n",
        "    publication = 'LE MATIN'\n",
        "    \n",
        "    if(date_publication <= date_max):\n",
        "        return False, date_publication, Id\n",
        "    Id = Id + 1\n",
        "    \n",
        "    #sentiment analysis\n",
        "    en_blob = TextBlob(str(content))\n",
        "    sent = en_blob.sentiment.polarity\n",
        "    if sent <-0.05 :\n",
        "        sentiment = \"Negative\"\n",
        "    elif sent > -0.05 and sent < 0.05 :\n",
        "        sentiment = \"Neutre\"\n",
        "    else :\n",
        "        sentiment = \"Positive\"\n",
        "\n",
        "    #Summarinzing\n",
        "    tokens = tokenizer(content)\n",
        "    sents = sent_tokenizer(content)\n",
        "    word_counts = count_words(tokens)\n",
        "    freq_dist = word_freq_distribution(word_counts)\n",
        "    sent_scores = score_sentences(sents, freq_dist)\n",
        "    summary, summary_sent_scores = summarize(sent_scores, 3)\n",
        "\n",
        "    writer.writerow({'Id': Id,'Title': title,'Author': author,'Date':date_publication,'Publication': publication,'URL': site,'Content': content,'Language': language,'Country': country,'sentiment':sentiment, 'Summary':summary})\n",
        "    return True, date_publication, Id\n",
        "\n",
        "def add_newArticles_LeMatin(Id, date_max):\n",
        "    p = 1\n",
        "    stay = True\n",
        "    fieldnames=['Id','Title','Author','Date','Publication','URL','Content','Language','Country','sentiment','Summary']\n",
        "    with open('Articles.csv','a+',encoding=\"utf-8\") as csv_file:\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "        lastDate= date_max\n",
        "        while stay:\n",
        "            site= \"https://lematin.ma/archives/maroc/\"+str(p)\n",
        "            p = p+1\n",
        "            hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "            req = Request(site,headers=hdr)\n",
        "            page = urlopen(req)\n",
        "            soup = BeautifulSoup(page)\n",
        "\n",
        "            links = soup.findAll(\"a\")\n",
        "            i=0\n",
        "            websites = []\n",
        "            for link in links:\n",
        "                try:\n",
        "                    res = link[\"href\"].split(\"/\")\n",
        "                    if res[0] == '':\n",
        "                        if len(res)>3 and str.isdigit(res[2]):\n",
        "                            i= i+1\n",
        "                            stay, date, Id = scrap_article_LeMatin(\"https://lematin.ma\"+link[\"href\"],date_max, Id, writer)\n",
        "                            if date > lastDate:\n",
        "                                lastDate = date\n",
        "                        if (len(res)>1 and res[1] == \"archives\") or not stay:\n",
        "                            break\n",
        "                except KeyError : #or urllib.error.HTTPError:\n",
        "                    pass\n",
        "            if i == 0:\n",
        "                stay = False\n",
        "    return Id, lastDate\n",
        "\n",
        "def update_LeMatin():\n",
        "    fieldnames=['country', 'Publication', 'maxId', 'lastDate']\n",
        "    with open('presse_lastUpdate.csv', newline='') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        readData = [row for row in reader]\n",
        "        found = False\n",
        "        id_max = 0 \n",
        "        lastdate = datetime(1 ,1 ,1 , 0, 0)\n",
        "        for row in readData:\n",
        "            if id_max < int(row['maxId']):\n",
        "                id_max = int(row['maxId'])\n",
        "            if row['country'] == \"Morocco\" and row['Publication'] == \"LE MATIN\" :\n",
        "                lastdate = pd.to_datetime(row['lastDate'])  \n",
        "                found = True\n",
        "       \n",
        "        id_max, lastdate = add_newArticles_LeMatin(id_max, lastdate)\n",
        "    if found :\n",
        "        with open('presse_lastUpdate.csv', \"w\", newline='') as file:\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for row in readData:\n",
        "                if row['country'] == \"Morocco\" and row['Publication'] == \"LE MATIN\" :\n",
        "                    row['maxId'] = id_max\n",
        "                    row['lastDate'] = lastdate\n",
        "                    break\n",
        "            writer.writerows(readData)\n",
        "    if not found:\n",
        "        with open('presse_lastUpdate.csv', \"a+\", newline='') as file:\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "            writer.writerow({'country' :'Morocco', 'Publication' : 'LE MATIN',  'maxId' : id_max, 'lastDate' :lastdate})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlKjvOmQ_AJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scrap_article_HespressFR(site, date_max, Id, writer):\n",
        "    #url = \"https://www.hespress.com/international/435842.html\"\n",
        "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "    req = Request(site,headers=hdr)\n",
        "    try :\n",
        "        page = urlopen(req)\n",
        "        soup = BeautifulSoup(page)\n",
        "\n",
        "        #title = soup.select('h1.page_title')[0].text.strip()\n",
        "        title = soup.find(\"div\", {\"class\": \"col article-container\"}).find('h1').text\n",
        "        if title.startswith(\"Températures minimales et maximales\") or title.startswith(\"Prévisions météorologiques pour la journée du \") or title.startswith(\"Revue de presse quotidienne\"):\n",
        "            return True, date_max, Id\n",
        "\n",
        "        content_a = soup.findAll(\"div\", {\"class\": \"content\"})\n",
        "        content = ''\n",
        "        for x in content_a:\n",
        "            for c in x.findAll('p'):\n",
        "                content += c.text + \"\\n\"\n",
        "        \n",
        "        if content == '':\n",
        "            return True, date_max, Id\n",
        "\n",
        "        date_author = soup.find(\"div\",{\"class\":\"ar-meta\"}).findAll(\"span\")\n",
        "        author = date_author[0].text.strip()\n",
        "        date_publication = dateparser.parse(date_author[1].text.lower().strip())\n",
        "\n",
        "        language='Fr'\n",
        "        country='Morocco'\n",
        "        publication='Hespress FR'\n",
        "\n",
        "\n",
        "        if(date_publication <= date_max):\n",
        "            return False, date_publication, Id \n",
        "        Id = Id+1\n",
        "\n",
        "        #sentiment analysis\n",
        "        en_blob = TextBlob(str(content))\n",
        "        sent = en_blob.sentiment.polarity\n",
        "        if sent <-0.05 :\n",
        "            sentiment = \"Negative\"\n",
        "        elif sent > -0.05 and sent < 0.05 :\n",
        "            sentiment = \"Neutre\"\n",
        "        else :\n",
        "            sentiment = \"Positive\"\n",
        "\n",
        "        #Summarinzing\n",
        "        tokens = tokenizer(content)\n",
        "        sents = sent_tokenizer(content)\n",
        "        word_counts = count_words(tokens)\n",
        "        freq_dist = word_freq_distribution(word_counts)\n",
        "        sent_scores = score_sentences(sents, freq_dist)\n",
        "        summary, summary_sent_scores = summarize(sent_scores, 3) \n",
        "\n",
        "        writer.writerow({'Id': Id,'Title': title,'Author': author,'Date':date_publication,'Publication': publication,'URL': site,'Content': content,'Language': language,'Country': country, 'sentiment':sentiment,'Summary':summary})\n",
        "        return True, date_publication, Id\n",
        "    except :\n",
        "        return True, date_max, Id\n",
        "\n",
        "\n",
        "def add_newArticles_hespressFR(Id, date_max):\n",
        "    p = 1\n",
        "    stay = True\n",
        "    fieldnames=['Id','Title','Author','Date', 'Publication','URL','Content','Language','Country','sentiment','Summary']\n",
        "    with open('Articles.csv','a+',encoding=\"utf-8\") as csv_file:\n",
        "        article_writer = csv.writer(csv_file)\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "        lastDate= date_max\n",
        "        while stay: \n",
        "            site= \"https://fr.hespress.com/tous-les-articles/page/\"+str(p)\n",
        "            p = p+1\n",
        "            hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "            req = Request(site,headers=hdr)\n",
        "            page = urlopen(req)\n",
        "            soup = BeautifulSoup(page)\n",
        "            \n",
        "            \n",
        "            links = soup.findAll(\"a\")\n",
        "            i=0\n",
        "            for link in links:\n",
        "                sLink = link[\"href\"].split(\"/\")\n",
        "                if len(sLink)>=4 and  str.isdigit(sLink[3].split(\"-\")[0]):\n",
        "                    stay, date, Id = scrap_article_HespressFR(link[\"href\"], date_max, Id, writer)\n",
        "                    if date > lastDate:\n",
        "                        lastDate = date\n",
        "                    if not stay:\n",
        "                        break\n",
        "                    i = i+1\n",
        "            if i == 0:\n",
        "                break\n",
        "    return Id,lastDate\n",
        "\n",
        "def update_hespressFR():\n",
        "    fieldnames=['country', 'Publication', 'maxId', 'lastDate']\n",
        "    with open('presse_lastUpdate.csv', newline='') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        readData = [row for row in reader]\n",
        "        found = False\n",
        "        id_max = 0 \n",
        "        lastdate = datetime(1 ,1 ,1 , 1, 1)\n",
        "       \n",
        "        for row in readData:\n",
        "            if id_max < int(row['maxId']):\n",
        "                id_max = int(row['maxId'])\n",
        "            if row['country'] == \"Morocco\" and row['Publication'] == \"Hespress FR\" :\n",
        "                lastdate = pd.to_datetime(row['lastDate'])  \n",
        "                found = True\n",
        "        id_max, lastdate = add_newArticles_hespressFR(id_max, lastdate)\n",
        "\n",
        "\n",
        "    if found :\n",
        "        with open('presse_lastUpdate.csv', \"w\", newline='') as file:\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for row in readData:\n",
        "                if row['country'] == \"Morocco\" and row['Publication'] == \"Hespress FR\" :\n",
        "                    row['maxId'] = id_max\n",
        "                    row['lastDate'] = lastdate\n",
        "                    break\n",
        "            writer.writerows(readData)\n",
        "    if not found:\n",
        "        with open('presse_lastUpdate.csv', \"a+\", newline='') as file:\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "            writer.writerow({'country' :'Morocco', 'Publication' : 'Hespress FR',  'maxId' : id_max, 'lastDate' :lastdate})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4Qp7Sfy4jDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "da8349de-21c1-4e5f-b25e-7aef5113270c"
      },
      "source": [
        "import threading \n",
        "  \n",
        "def hespressFR(): \n",
        "    update_hespressFR()\n",
        "  \n",
        "def leMatin(): \n",
        "    update_LeMatin()\n",
        "  \n",
        "if __name__ == \"__main__\": \n",
        "    # creating thread \n",
        "    t1 = threading.Thread(target=hespressFR) \n",
        "    t2 = threading.Thread(target=leMatin) \n",
        "    \n",
        "    t1.start()\n",
        "    t2.start() \n",
        "\n",
        "    t1.join()\n",
        "    t2.join()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-6:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-13-e2e89b82ab82>\", line 7, in leMatin\n",
            "    update_LeMatin()\n",
            "  File \"<ipython-input-10-13d8a0f196b0>\", line 104, in update_LeMatin\n",
            "    id_max, lastdate = add_newArticles_LeMatin(id_max, lastdate)\n",
            "  File \"<ipython-input-10-13d8a0f196b0>\", line 78, in add_newArticles_LeMatin\n",
            "    stay, date, Id = scrap_article_LeMatin(\"https://lematin.ma\"+link[\"href\"],date_max, Id, writer)\n",
            "  File \"<ipython-input-10-13d8a0f196b0>\", line 5, in scrap_article_LeMatin\n",
            "    page = urlopen(req)\n",
            "  File \"/usr/lib/python3.6/urllib/request.py\", line 223, in urlopen\n",
            "    return opener.open(url, data, timeout)\n",
            "  File \"/usr/lib/python3.6/urllib/request.py\", line 532, in open\n",
            "    response = meth(req, response)\n",
            "  File \"/usr/lib/python3.6/urllib/request.py\", line 642, in http_response\n",
            "    'http', request, response, code, msg, hdrs)\n",
            "  File \"/usr/lib/python3.6/urllib/request.py\", line 570, in error\n",
            "    return self._call_chain(*args)\n",
            "  File \"/usr/lib/python3.6/urllib/request.py\", line 504, in _call_chain\n",
            "    result = func(*args)\n",
            "  File \"/usr/lib/python3.6/urllib/request.py\", line 650, in http_error_default\n",
            "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
            "urllib.error.HTTPError: HTTP Error 404: Not Found\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}