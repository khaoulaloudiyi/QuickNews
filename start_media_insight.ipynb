{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "start media insight.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaoulaloudiyi/QuickNews/blob/master/start_media_insight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfX_XtRCW8R2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmeX0F2y4HIu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "922e5f83-750e-4abe-9240-74d7162a3cff"
      },
      "source": [
        "cd /content/drive/My Drive/MediaInsight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1slwd6zf9z3HwT_o9xa6gz2ym2VnOfDD8/MediaInsights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNxbiCWtj_3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7cea7d77-11b6-4dda-cba6-a654a663a752"
      },
      "source": [
        "!pip install BeautifulSoup4\n",
        "!pip install pandas\n",
        "!pip install datetime\n",
        "!pip install dateparser\n",
        "!pip install flask_ngrok\n",
        "!pip install googletrans\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
            "Collecting datetime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/22/a5297f3a1f92468cc737f8ce7ba6e5f245fcfafeae810ba37bd1039ea01c/DateTime-4.3-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.8MB/s \n",
            "\u001b[?25hCollecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from datetime) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface->datetime) (49.6.0)\n",
            "Installing collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-4.3 zope.interface-5.1.0\n",
            "Collecting dateparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/d5/5a2e51bc0058f66b54669735f739d27afc3eb453ab00520623c7ab168e22/dateparser-0.7.6-py2.py3-none-any.whl (362kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from dateparser) (2.8.1)\n",
            "Requirement already satisfied: regex!=2019.02.19 in /usr/local/lib/python3.6/dist-packages (from dateparser) (2019.12.20)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from dateparser) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->dateparser) (1.15.0)\n",
            "Installing collected packages: dateparser\n",
            "Successfully installed dateparser-0.7.6\n",
            "Collecting flask_ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask_ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask_ngrok) (1.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (2.10)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask_ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Collecting googletrans\n",
            "  Downloading https://files.pythonhosted.org/packages/71/3a/3b19effdd4c03958b90f40fe01c93de6d5280e03843cc5adf6956bfc9512/googletrans-3.0.0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.6.20)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/82/4bd4b7d9c0d1dc0fbfbc2a1e00138e7f3ab85bc239358fe9b78aa2ab586d/sniffio-1.1.0-py3-none-any.whl\n",
            "Collecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/07/12dd706501a3212a9774feb69d6a2333963a2da19ba98861ab23f2439f3d/hstspreload-2020.9.9-py3-none-any.whl (953kB)\n",
            "\u001b[K     |████████████████████████████████| 962kB 7.7MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.5MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.4MB/s \n",
            "\u001b[?25hCollecting contextvars>=2.1; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: googletrans, contextvars\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-cp36-none-any.whl size=15736 sha256=a481bdcffddf1f165b387e06b4dabd0523cc5b1b060089698ea7d295010267a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/1a/a7/eaf4d7a3417a0c65796c547cff4deb6d79c7d14c2abd29273e\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=1d980411221c97f41bc92abade6278bbda0cab7c97a578dc6063f1490b10e2ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built googletrans contextvars\n",
            "Installing collected packages: rfc3986, h11, hyperframe, hpack, h2, immutables, contextvars, sniffio, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed contextvars-2.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.9.9 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 immutables-0.14 rfc3986-1.4.0 sniffio-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFhKq155RJ0w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "43a8ed3f-05fa-40cf-8629-43a1ffa6a4e1"
      },
      "source": [
        "from collections import Counter \n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stop_words\n",
        "import spacy \n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import nltk \n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "swa = stopwords.words('arabic')\n",
        "swf = set(stopwords.words('french'))\n",
        "sws = set(stopwords.words('spanish'))\n",
        "#tokenizing sentences into words\n",
        "def tokenizer(s):\n",
        "    tokens = []\n",
        "    for word in s.split(' '):\n",
        "        tokens.append(word.strip().lower()) #strip remove spaces at the beginning and at the end of the string\n",
        "    return tokens\n",
        "#tokenizing document into sentences \n",
        "\n",
        "def sent_tokenizer(s):\n",
        "    sents = []\n",
        "    for sent in s.split('.'):\n",
        "        sents.append(sent.strip())\n",
        "    return sents\n",
        "\n",
        "\n",
        "def get_content(url):\n",
        "  req_obj = requests.get(url)\n",
        "  text = req_obj.text\n",
        "  soup = BeautifulSoup(text)\n",
        "  title = soup.find(\"div\" , {\"class\" : \"col article-container\"})\n",
        "  content_a = soup.findAll(\"div\", {\"class\": \"content\"})\n",
        "  content = ''\n",
        "  for x in content_a:\n",
        "    for c in x.findAll('p'):\n",
        "      content += c.text + \"\\n\"\n",
        "  return content\n",
        "\n",
        "\n",
        "def count_words(tokens):\n",
        "    word_counts = {}\n",
        "    for token in tokens:\n",
        "        if token not in stop_words and token not in STOP_WORDS and token not in swa and token not in sws and token not in swf and token not in punctuation:\n",
        "            if token not in word_counts.keys():\n",
        "                word_counts[token] = 1\n",
        "            else:\n",
        "                word_counts[token] += 1\n",
        "    return word_counts\n",
        "\n",
        "\n",
        "\n",
        "def word_freq_distribution(word_counts):\n",
        "    freq_dist = {}\n",
        "    max_freq = max(word_counts.values())\n",
        "    for word in word_counts.keys():  \n",
        "        freq_dist[word] = (word_counts[word]/max_freq)\n",
        "    return freq_dist\n",
        "\n",
        "def score_sentences(sents, freq_dist, max_len=40):\n",
        "    sent_scores = {}  \n",
        "    for sent in sents:\n",
        "        words = sent.split(' ')\n",
        "        for word in words:\n",
        "            if word.lower() in freq_dist.keys():\n",
        "                if len(words) < max_len:\n",
        "                    if sent not in sent_scores.keys():\n",
        "                        sent_scores[sent] = freq_dist[word.lower()]\n",
        "                    else:\n",
        "                        sent_scores[sent] += freq_dist[word.lower()]\n",
        "    return sent_scores\n",
        "\n",
        "\n",
        "def summarize(sent_scores, k):\n",
        "    top_sents = Counter(sent_scores) \n",
        "    summary = ''\n",
        "    scores = []\n",
        "    \n",
        "    top = top_sents.most_common(k)\n",
        "    for t in top: \n",
        "        summary += t[0].strip()+'. '\n",
        "        scores.append((t[1], t[0]))\n",
        "    return summary[:-1], scores\n",
        "\n",
        "\n",
        "def function(url_content):\n",
        "  tokens = tokenizer(url_content)\n",
        "  sents = sent_tokenizer(url_content)\n",
        "  word_counts = count_words(tokens)\n",
        "  freq_dist = word_freq_distribution(word_counts)\n",
        "  sent_scores = score_sentences(sents, freq_dist)\n",
        "  summary, summary_sent_scores = summarize(sent_scores, 3)\n",
        "  return summary\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YcpCYmtLkVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from googletrans import Translator\n",
        "translator = Translator()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEVBl9eFjkeb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "503ee406-9cb8-4b34-99c8-3b0d5399fe0b"
      },
      "source": [
        " #pip install BeautifulSoup4, pandas, datetime, dateparser\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from flask import Flask, request, render_template, g \n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask_ngrok import *\n",
        "from urllib.request import Request, urlopen\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import csv\n",
        "import dateparser\n",
        "import sqlite3\n",
        "import json \n",
        "import re\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  # Start ngrok when app is run\n",
        "article=\"\"\n",
        "\n",
        "#recherche 1\n",
        "def score_content(my_theme, content):\n",
        "    result = []\n",
        "    vectorizer = TfidfVectorizer(stop_words = \"english\", use_idf = True, smooth_idf = True)\n",
        "    vectorizer.fit(content+my_theme)\n",
        "    vector_list_contents = vectorizer.transform(content)\n",
        "    vector_my_theme = vectorizer.transform(my_theme)\n",
        "    result = linear_kernel(vector_my_theme, vector_list_contents)\n",
        "    score=sum(result)/len(result)\n",
        "    return score\n",
        "\n",
        "def Recommendation(my_theme, list_content, dateFrom, dateTo):\n",
        "    themes_publishers = []\n",
        "    for k in range(0, len(list_content['Id'])):\n",
        "        S= [1]\n",
        "        if dateFrom or dateTo:\n",
        "            if dateFrom and dateparser.parse(dateFrom)> dateparser.parse(list_content['Date'][k]):\n",
        "                S= [0]\n",
        "            if dateTo and dateparser.parse(dateTo)< dateparser.parse(list_content['Date'][k]):\n",
        "                S= [0]\n",
        "        if S[0] == 1:\n",
        "            S = score_content([my_theme], [list_content['Content'][k]])\n",
        "        themes_publishers.append(S[0])\n",
        "    list_content['score']=themes_publishers \n",
        "    list_content.sort_values(by=[\"score\"] , inplace=True , ascending = False )\n",
        "    \n",
        "    articles_result = []\n",
        "    artic = pd.DataFrame(columns=['Id','Title','Author','Date', 'Publication','URL','Content','Language','Country','sentiment','Summary'])\n",
        "    if(list_content.loc[list_content.index[0],\"score\"]< 0.001):\n",
        "        return articles_result, False\n",
        "    else:\n",
        "        #print(\"\\nLes TOP articles traitants votre theme sont :\\n\")\n",
        "        e = 0\n",
        "        for i, element in list_content.iterrows():\n",
        "            if(element[\"score\"] >= 0.001) or e==50:\n",
        "                my_dict = {'publication' : element['Publication'], 'titre' : element['Title'], 'date' : element['Date'], 'content' : element['Content'][0:140]+\"...\", 'author' : element['Author'], 'URL': element['URL'], 'sentiment' : element['sentiment'], 'summary': element['Summary']}\n",
        "                articles_result.append(my_dict)\n",
        "                artic = artic.append(element, ignore_index=True)\n",
        "                e = e+1\n",
        "            else:\n",
        "                break    \n",
        "    return articles_result, artic\n",
        "\n",
        "def research_by_similarity(my_theme, dateFrom, dateTo):\n",
        "    global article\n",
        "    list_content=pd.read_csv(article)\n",
        "    list_content = list_content #[:100]\n",
        "    return Recommendation(my_theme, list_content, dateFrom, dateTo)\n",
        "\n",
        "#recherche2\n",
        "def syntaxique_research(list_inputs,dateFrom, dateTo):\n",
        "    data = pd.read_csv(article, low_memory=False, encoding='utf-8')\n",
        "    answer=False\n",
        "    articles = []\n",
        "    if dateFrom :\n",
        "            dateF = dateparser.parse(dateFrom)\n",
        "    if dateTo:\n",
        "            dateT = dateparser.parse(dateTo) \n",
        "    artic = pd.DataFrame(columns=['Id','Title','Author','Date', 'Publication','URL','Content','Language','Country','sentiment','Summary']) \n",
        "    for i in range(len(data.iloc[:,2])):\n",
        "        if dateFrom :\n",
        "            if dateF > dateparser.parse(data.loc[i,'Date']):\n",
        "                continue\n",
        "        if dateTo:\n",
        "            if  dateT< dateparser.parse(data.loc[i,'Date']):\n",
        "                continue\n",
        "        for j in list_inputs:\n",
        "            e=j.lower()\n",
        "            if type(data.loc[i,'Content']) is str:\n",
        "                res = data.loc[i,'Content'].lower().find(e)\n",
        "            else :\n",
        "                res = -1\n",
        "\n",
        "            if res != -1 :\n",
        "                my_dict = {'publication' : data.loc[i,'Publication'], 'titre' : data.loc[i,'Title'], 'date' : data.loc[i,'Date'], 'content' : data.loc[i,'Content'][0:140]+\"...\", 'author' : data.loc[i,'Author'], 'URL': data.loc[i,'URL'], 'sentiment' : data.loc[i,'sentiment'], 'summary': data.loc[i,'Summary'] }\n",
        "                articles.append(my_dict)\n",
        "                artic = artic.append(data.loc[i], ignore_index=True)\n",
        "                answer= True\n",
        "                break\n",
        "    return articles, artic\n",
        "\n",
        "def pre_process(text):\n",
        "    \n",
        "    text=text.lower()\n",
        "    text=re.sub(\"</?.*?>\",\" <> \",text)\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    return text\n",
        "\n",
        "def get_stop_words():\n",
        "    #with open('stop_words.csv', 'r', encoding=\"utf-8\") as f:\n",
        "    data = pd.read_csv('stopWords.csv', low_memory=False, encoding='utf-8')\n",
        "    stopwords = data.loc[:,\"stop_words\"]\n",
        "    stop_set = set(m.strip() for m in stopwords)\n",
        "    return frozenset(stop_set)\n",
        "\n",
        "\n",
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        fname = feature_names[idx]\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    return results\n",
        "\n",
        "def get_Tendance(articles):\n",
        "    my_list = articles['Content'].tolist()\n",
        "    final_element = ''\n",
        "    #gather all the elements in one element\n",
        "    for i in range(len(my_list)):\n",
        "        final_element = final_element + \" \"+ pre_process(my_list[i])\n",
        "    # get all the stop_words from spanish, english, frensh and arabic\n",
        "    stopwords=get_stop_words()\n",
        "    #generate tf-idf for the given document\n",
        "    vectorizer = TfidfVectorizer(stop_words = stopwords, use_idf=True, smooth_idf=True)\n",
        "    cv = vectorizer.fit(my_list)\n",
        "    feature_names = cv.get_feature_names()\n",
        "    tf_idf_vector = vectorizer.transform([final_element])\n",
        "\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "    #extract the top n keywords\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "\n",
        "    #results\n",
        "    keyWordList = []\n",
        "    for k in keywords:\n",
        "        myDict = {\"Name\":k, \"Count\":int(keywords[k]*10000)}\n",
        "        #word = []\n",
        "        #word.append(str(k))\n",
        "        #word.append(int(keywords[k]*1000))\n",
        "        keyWordList.append(myDict)\n",
        "    keyWords ={\"children\": keyWordList}\n",
        "    return  keyWords\n",
        "\n",
        "@app.route('/research', methods=['POST'])\n",
        "def get_articles():\n",
        "    global article\n",
        "    dateFrom = request.form['from']\n",
        "    dateTo = request.form['to']\n",
        "    list_inputs = [\" \"+a.strip().lower()+\" \" for a in request.form['keyword'].split(\"-\")]\n",
        "    if request.form['gridRadios1']  == 'option21':\n",
        "      article=\"ArticlesEP.csv\"\n",
        "    elif request.form['gridRadios1']  == 'option31':\n",
        "      article=\"ArticlesAR.csv\"\n",
        "    elif request.form['gridRadios1']  == 'option41':\n",
        "      article=\"Articles.csv\"\n",
        "    else:\n",
        "      article=\"ArticlesENG.csv\"\n",
        "    if request.form['gridRadios']  == 'option1':\n",
        "      articles, artic = syntaxique_research(list_inputs, dateFrom, dateTo)\n",
        "    else :\n",
        "      articles, artic = research_by_similarity(\" \".join(list_inputs), dateFrom, dateTo)\n",
        "    if not artic.empty:\n",
        "        maxD = {}\n",
        "        values, keys =list(artic.groupby('Country')['Country'].transform('count')), artic['Country'].tolist()\n",
        "        countrydictionary = dict(zip(keys, values))\n",
        "        maxD[\"max\"] =  sum(countrydictionary.values())\n",
        "        \n",
        "        authordictionary= {}\n",
        "        values, keys =list(artic.groupby('Author')['Author'].transform('count')), artic['Author'].tolist()\n",
        "        authordictionary = dict(zip(keys, values))\n",
        "        sorted_author = sorted(authordictionary.items(), key=lambda kv: kv[1], reverse=True)\n",
        "        sumAuthor = sum(authordictionary.values())\n",
        "        authorList = []\n",
        "        count = 0\n",
        "        for i in range(0,min(9,len(sorted_author))):\n",
        "            authorList.append(sorted_author[i])\n",
        "            count += sorted_author[i][1]\n",
        "        authorList.append([\"others\", sumAuthor - count])\n",
        "        authordictionary[\"authors\"] = authorList\n",
        "\n",
        "        values, keys =list(artic.groupby('sentiment')['sentiment'].transform('count')), artic['sentiment'].tolist()\n",
        "        sentimentdictionary = dict(zip(keys, values)) \n",
        "    \n",
        "        #values, keys =list(artic.groupby('summary')['summary'].transform('count')), artic['summary'].tolist()\n",
        "        #summarydictionary = dict(zip(keys, values))  \n",
        "\n",
        "        tendance= get_Tendance(artic)\n",
        "        \n",
        "        message = \"Le nombre de résultat est : \"+str(len(articles))\n",
        "        showRes = \"true\"\n",
        "        \n",
        "        return render_template('Show_stat.html', countrydict = json.dumps(countrydictionary), authordict=json.dumps(authordictionary), sentimentdict=json.dumps(sentimentdictionary), maxD = json.dumps(maxD), articles=articles, num = message, keyWords = json.dumps(tendance), showRes = showRes)\n",
        "    else:\n",
        "        message = \"Out of InSight \"\n",
        "        showRes = \"false\"\n",
        "        return render_template('Show_stat.html', countrydict = json.dumps({}), authordict=json.dumps({}), sentimentdict=json.dumps({}), maxD = json.dumps(0), articles=[], num = message, keyWords = json.dumps({}), showRes = showRes)\n",
        "\n",
        "\n",
        "    #return render_template('search_presse.html', articles=articles, num = \"Le nombre de résultat est : \"+str(len(articles)))\n",
        "\n",
        "@app.route('/') \n",
        "def get_root():\n",
        "    return render_template('index1.html')\n",
        "\n",
        "@app.route('/SignUp.html')\n",
        "def get_sign2():\n",
        "    return render_template('SignUp.html')\n",
        "\n",
        "@app.route('/Login.html')\n",
        "def get_login():\n",
        "    return render_template('Login.html')\n",
        "\n",
        "@app.route('/Show_stat.html')\n",
        "def get_stat():\n",
        "    return render_template('Show_stat.html')\n",
        "\n",
        "@app.route('/accueil')\n",
        "def get_ac():\n",
        "    return render_template('index1.html')\n",
        "\n",
        "\n",
        "@app.route('/search_presse.html')\n",
        "def get_research_():\n",
        "    return render_template('search_presse.html')\n",
        "\n",
        "\n",
        "@app.route('/index.html',methods=['GET', 'POST'])\n",
        "def get_resume():\n",
        "  if request.method == \"POST\": \n",
        "    url = request.form.get(\"url\")\n",
        "    summary = request.form.get(\"summary\")\n",
        "\n",
        "    summary_ar = (translator.translate(summary,dest='ar')).text\n",
        "    summary_en = (translator.translate(summary,dest='en')).text\n",
        "    summary_sp = (translator.translate(summary,dest='es')).text\n",
        "    summary_fr = (translator.translate(summary,dest='fr')).text\n",
        "\n",
        "    publication = request.form.get(\"publication\")\n",
        "    titre = request.form.get(\"titre\")\n",
        "\n",
        "    \n",
        "  return render_template('index.html',summary=summary,publication=publication,titre=titre)\n",
        "  \n",
        "\n",
        " \n",
        "    \n",
        "@app.route('/existing_account', methods=['POST'])\n",
        "def existing_account():\n",
        "    g.db = sqlite3.connect('library.db')\n",
        "    users = g.db.execute(\"SELECT name FROM Users where name ='\"+request.form['name']+\"' and password = '\"+request.form['pass']+\"';\")\n",
        "    exist = False\n",
        "    for user in users.fetchall():\n",
        "        exist = True\n",
        "    if not exist:\n",
        "        return render_template('Login.html', message = \"Compte n'existe pas\")\n",
        "    else:\n",
        "        return render_template('search_presse.html')\n",
        "\n",
        "@app.route('/new_account', methods=['POST'])\n",
        "def new_account():\n",
        "    g.db = sqlite3.connect('library.db')\n",
        "    users = g.db.execute(\"SELECT name FROM Users where name ='\"+request.form['name']+\"' and password = '\"+request.form['pass']+\"';\")\n",
        "    exist = False\n",
        "    for user in users.fetchall():\n",
        "        exist = True\n",
        "    if not exist:\n",
        "        if request.form['pass'] == request.form['re_pass'] :\n",
        "            g.db.execute(\"Insert Into Users(name, password, email) Values('\"+ request.form['name'] +\"','\"+ request.form['pass'] +\"','\"+request.form['email'] +\"');\")\n",
        "            g.db.commit()\n",
        "            return render_template('search_presse.html')\n",
        "        else :\n",
        "            return render_template('SignUp.html', message = \"Le mot de passe doit être <br> identique !!!\")\n",
        "    else:\n",
        "        return render_template('SignUp.html', message = \"User name existe déja\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  print(\"Start running server\")\n",
        "  app.run()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start running server\n",
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://4333ca9d88b5.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [14/Sep/2020 13:49:52] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:50:00] \"\u001b[37mGET /static/img/core-img/logo.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:50:00] \"\u001b[37mGET /static/img/bg-img/18.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:50:00] \"\u001b[37mGET /static/img/bg-img/17.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:50:00] \"\u001b[37mGET /static/img/bg-img/16.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:50:09] \"\u001b[37mGET /search_presse.html HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:50:18] \"\u001b[37mGET /static/style.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:50:20] \"\u001b[37mGET /static/img/core-img/logo.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:52:40] \"\u001b[37mGET /static/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:52:41] \"\u001b[37mGET /static/css/animate.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:52:41] \"\u001b[37mGET /static/css/default-assets/classy-nav.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:52:42] \"\u001b[37mGET /static/js/popper.min.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:52:42] \"\u001b[37mGET /static/js/bootstrap.min.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:52:42] \"\u001b[37mGET /static/js/jquery.min.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:53:07] \"\u001b[37mGET /static/css/magnific-popup.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:53:08] \"\u001b[37mGET /static/css/owl.carousel.min.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:53:16] \"\u001b[37mGET /static/css/nice-select.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:53:16] \"\u001b[37mGET /static/css/font-awesome.min.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:53:16] \"\u001b[37mGET /static/css/style.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:53:17] \"\u001b[37mGET /static/css/bootstrap-datepicker.min.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:54:36] \"\u001b[37mGET /static/css/jquery-ui.min.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:54:36] \"\u001b[37mGET /static/js/roberto.bundle.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:54:36] \"\u001b[37mGET /static/js/default-assets/active.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 13:55:18] \"\u001b[37mGET /static/img/bg-img/image3.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 14:02:27] \"\u001b[37mGET /static/css/fonts/ElegantIcons.woff HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 14:02:28] \"\u001b[37mGET /static/fonts/fontawesome-webfont.woff2?v=4.7.0 HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 14:04:23] \"\u001b[33mGET /static/img/core-img/favicon.png HTTP/1.1\u001b[0m\" 404 -\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'quelqu', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn', 'آمين', 'آها', 'أب', 'أخ', 'أف', 'أفعل', 'أفعله', 'ؤلاء', 'إل', 'إليك', 'إليكن', 'إم', 'إيه', 'ات', 'اتان', 'ارتد', 'انفك', 'بخ', 'برح', 'بس', 'تان', 'تبد', 'تحو', 'تعل', 'حد', 'حم', 'حي', 'خب', 'ذار', 'ذان', 'سيما', 'شتان', 'صه', 'ظن', 'عد', 'قط', 'كأي', 'مر', 'مكان', 'مكانكن', 'نب', 'هات', 'هاك', 'هب', 'واها', 'وراء'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "127.0.0.1 - - [14/Sep/2020 16:01:43] \"\u001b[37mPOST /research HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 16:01:50] \"\u001b[37mGET /static/js/d3pie.min.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 16:01:51] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [14/Sep/2020 16:02:00] \"\u001b[37mPOST /index.html HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [14/Sep/2020 16:02:05] \"\u001b[33mGET /nicepage.css HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [14/Sep/2020 16:02:05] \"\u001b[33mGET /Page-1.css HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [14/Sep/2020 16:02:05] \"\u001b[33mGET /jquery.js HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [14/Sep/2020 16:02:05] \"\u001b[33mGET /nicepage.js HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [14/Sep/2020 16:02:05] \"\u001b[33mGET /images/60077728-0.jpeg HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uffsuFHOxV_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install --upgrade git+https://github.com/dbcli/mycli.git"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}