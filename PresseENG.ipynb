{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "PresseENG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaoulaloudiyi/QuickNews/blob/master/PresseENG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHdpHSWd_VcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "5304fe2d-f416-4766-dc71-0a6c75f9bb53"
      },
      "source": [
        "!pip install dateparser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dateparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/d5/5a2e51bc0058f66b54669735f739d27afc3eb453ab00520623c7ab168e22/dateparser-0.7.6-py2.py3-none-any.whl (362kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 368kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser) (1.5.1)\n",
            "Requirement already satisfied: regex!=2019.02.19 in /usr/local/lib/python3.6/dist-packages (from dateparser) (2019.12.20)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from dateparser) (2018.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from dateparser) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->dateparser) (1.15.0)\n",
            "Installing collected packages: dateparser\n",
            "Successfully installed dateparser-0.7.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cuLXKie_f42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b6c2005e-d60b-4e42-d39f-cce4c9105ff7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/4AH8vcroxUol89Vb11m3v6GRXVhEijT0kp4w_3g7fs69ZC24wXoMZa8\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vun791UC_F-z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "457a885d-b9e8-400b-9508-031d66278aef"
      },
      "source": [
        "cd /content/drive/My Drive//MediaInsights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MediaInsights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMJcyVGeINXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01c6fe9c-af70-42f9-948f-e54c54cb4b1a"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MediaInsights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwHQAk6z_AIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "68d32255-4b46-4e62-af20-080d3a5e9db1"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import csv\n",
        "import dateparser\n",
        "from textblob import TextBlob\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from collections import Counter \n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stop_words\n",
        "import spacy \n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import nltk \n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrFuJaLbH02K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b37aab06-5921-41f4-f410-67dc52562746"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "swa = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kolBwjDQ_AIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cette partie doit être executer une seule fois pour créer le header des deux fichiers csv\n",
        "\n",
        "#Ce fichier joue un rôle principal pour la mise à jour puisqu'il contient le champ lastDate qui réfère à la date ou la dernière mise à jour a été effectué et le champ maxid pour faciliter la recherche du plus grand Id \n",
        "with open('presse_lastUpdate.csv','w',encoding=\"utf-8\") as csv_file:\n",
        "    fieldnames=['country', 'Publication', 'maxId', 'lastDate']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    \n",
        "#Ce fichier est réservé pour stocker les différents articles récupéré grâce au scraping \n",
        "with open('ArticlesENG.csv','w',encoding=\"utf-8\") as csv_file:\n",
        "    fieldnames=['Id','Title','Author','Date','Publication','URL','Content','Language','Country', \"sentiment\", \"Summary\"]\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-ppz8Td_AI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#D'abord il faut comprendre la structure des sites web de la presse, chaque presse possède un archive où tous les articles sont stocké, sous forme d'une dizaine d'articles dans plusieurs pages, \n",
        "#chaque page de ce dernier contient plusieurs articles référencé par des liens, et tous ces articles possède le même format et contient l'ensemble des informations sauf \n",
        "#l'information sur le \"sentiment\"\n",
        "\n",
        "#Le principe du Scraping qu'on a utilisé est le suivant \"exemple avec Hespress Ar mais la même chose se répète avec les autres presses\":\n",
        "#d'abord on effectue une étude pour comprendre la structure d'une page de l'archive (code html) afin de récupérer les liens des articles seulement\n",
        "#apres on analyse la structure d'un de ces articles (html) , On parcour l'ensemble des pages de l'archive grâce à la fonction \"add_newArticles_hespres\" cette\n",
        "#fonction utilise la variable p qui représente le numéro de page de l'archive a scrapé, on scrape cette page puis on collecte les liens\n",
        "#href des articles et pour chaque lien on scrape l'article avec la fonction \"scrap_article_Hespress\" et on l'ajout à la dataset Articles\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KEjmiuXIcih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenizing sentences into words\n",
        "def tokenizer(s):\n",
        "    tokens = []\n",
        "    for word in s.split(' '):\n",
        "        tokens.append(word.strip().lower()) #strip remove spaces at the beginning and at the end of the string\n",
        "    return tokens\n",
        "\n",
        "#tokenizing document into sentences \n",
        "def sent_tokenizer(s):\n",
        "    sents = []\n",
        "    for sent in s.split('.'):\n",
        "        sents.append(sent.strip())\n",
        "    return sents\n",
        "\n",
        "def count_words(tokens):\n",
        "    word_counts = {}\n",
        "    for token in tokens:\n",
        "        if token not in stop_words and token not in STOP_WORDS and token not in swa and token not in punctuation:\n",
        "            if token not in word_counts.keys():\n",
        "                word_counts[token] = 1\n",
        "            else:\n",
        "                word_counts[token] += 1\n",
        "    return word_counts\n",
        "\n",
        "def word_freq_distribution(word_counts):\n",
        "    freq_dist = {}\n",
        "    max_freq = max(word_counts.values())\n",
        "    for word in word_counts.keys():  \n",
        "        freq_dist[word] = (word_counts[word]/max_freq)\n",
        "    return freq_dist\n",
        "\n",
        "def score_sentences(sents, freq_dist, max_len=40):\n",
        "    sent_scores = {}  \n",
        "    for sent in sents:\n",
        "        words = sent.split(' ')\n",
        "        for word in words:\n",
        "            if word.lower() in freq_dist.keys():\n",
        "                if len(words) < max_len:\n",
        "                    if sent not in sent_scores.keys():\n",
        "                        sent_scores[sent] = freq_dist[word.lower()]\n",
        "                    else:\n",
        "                        sent_scores[sent] += freq_dist[word.lower()]\n",
        "    return sent_scores\n",
        "\n",
        "def summarize(sent_scores, k):\n",
        "    top_sents = Counter(sent_scores) \n",
        "    summary = ''\n",
        "    scores = []\n",
        "    \n",
        "    top = top_sents.most_common(k)\n",
        "    for t in top: \n",
        "        summary += t[0].strip()+'. '\n",
        "        scores.append((t[1], t[0]))\n",
        "    return summary[:-1], scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8d37kgr_AJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scrap_article_TheAtlantic(site,date_max,Id,writer):\n",
        "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "    req = Request(site,headers=hdr)\n",
        "    page = urlopen(req)\n",
        "    soup = BeautifulSoup(page)\n",
        "\n",
        "   # try:\n",
        "    title_test = soup.select('h1.c-article-header__hed')\n",
        "    if not title_test :\n",
        "        return True, date_max, Id\n",
        "    title =title_test[0].text.strip()\n",
        "        \n",
        "    #author=soup.select('a.c-byline__link')[0].text.strip()\n",
        "    author = \"The Atlantic\"\n",
        "    aut =soup.select('a.c-byline__link') \n",
        "    if aut :\n",
        "        author = aut[0].text.strip()\n",
        "\n",
        "    date_pub=soup.select('time.c-dateline')[0].get('datetime')\n",
        "    date = date_pub.split(\"T\")\n",
        "    date1 = date[1].split(\"-\")\n",
        "    v_date=date[0]\n",
        "    date_publication =  pd.to_datetime(date[0]+\" \"+date1[0])\n",
        "    language='En'\n",
        "    country='United States'\n",
        "    publication='The Atlantic'\n",
        "    content=\"\\n\"\n",
        "    scrap_content = soup.findAll(\"section\", {\"class\": \"l-article__section s-cms-content\"})\n",
        "    for x in scrap_content:\n",
        "        for c in x.findAll('p'):\n",
        "            content += c.text+\"\\n\"\n",
        "\n",
        "    if(date_publication <= date_max):\n",
        "        return False, date_publication, Id\n",
        "\n",
        "    Id = Id + 1\n",
        "    en_blob = TextBlob(str(content))\n",
        "    sent = en_blob.sentiment.polarity\n",
        "    if sent <-0.05 :\n",
        "        sentiment = \"Negative\"\n",
        "    elif sent > -0.05 and sent < 0.05 :\n",
        "        sentiment = \"Neutre\"\n",
        "    else :\n",
        "        sentiment = \"Positive\"\n",
        "    \n",
        "    #Summarinzing\n",
        "    tokens = tokenizer(content)\n",
        "    sents = sent_tokenizer(content)\n",
        "    word_counts = count_words(tokens)\n",
        "    freq_dist = word_freq_distribution(word_counts)\n",
        "    sent_scores = score_sentences(sents, freq_dist)\n",
        "    summary, summary_sent_scores = summarize(sent_scores, 4)\n",
        "\n",
        "    writer.writerow({'Id': Id,'Title': title,'Author': author,'Date':date_publication,'Publication': publication,'URL': site,'Content': content,'Language': language,'Country': country, 'sentiment' : sentiment, 'Summary':summary}) \n",
        "    return True, date_publication, Id\n",
        "\n",
        "\n",
        "def Add_new_articles_TheAtlantic(Id,date_max):\n",
        "    stay=True\n",
        "    p=1\n",
        "    \n",
        "    with open('ArticlesENG.csv','a+',encoding=\"utf-8\") as csv_file:\n",
        "        fieldnames=['Id','Title','Author','Date','Publication','URL','Content','Language','Country','sentiment','Summary']\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "        lastDate= date_max\n",
        "        try:\n",
        "            while stay:\n",
        "            \n",
        "                link= \"https://www.theatlantic.com/latest/?page=\"+str(p)\n",
        "                p = p+1\n",
        "                hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "                req = Request(link,headers=hdr)\n",
        "                page = urlopen(req)\n",
        "                soup = BeautifulSoup(page)\n",
        "                \n",
        "                stay = False \n",
        "                links =  soup.findAll(\"a\")\n",
        "                for link in links:\n",
        "                    res = link[\"href\"].split('/')\n",
        "                    if len(res)>3 and res[2] == \"archive\" and str.isdigit(res[3]):\n",
        "                        site=\"https://www.theatlantic.com\"+link[\"href\"]                        \n",
        "                        stay, date, Id = scrap_article_TheAtlantic(site, date_max, Id, writer)\n",
        "                        if date > lastDate:\n",
        "                            lastDate = date\n",
        "                        if not stay :\n",
        "                            break\n",
        "                        stay = True\n",
        "                \n",
        "        except urllib.error.HTTPError:\n",
        "            stay = False\n",
        "    return Id, lastDate\n",
        "                \n",
        "def updates_TheAtlantic():\n",
        "    fieldnames=['country', 'Publication', 'maxId', 'lastDate']\n",
        "    with open('presse_lastUpdate.csv', newline='') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        readData = [row for row in reader]\n",
        "        found = False\n",
        "        id_max = 0 \n",
        "        lastdate = datetime(1 ,1 ,1 , 1, 1)\n",
        "        for row in readData:\n",
        "            if id_max < int(row['maxId']):\n",
        "                id_max = int(row['maxId'])\n",
        "            if row['country'] == \"US\" and row['Publication'] == \"The Atlantic\" :\n",
        "                lastdate = pd.to_datetime(row['lastDate'])  \n",
        "                found = True\n",
        "\n",
        "        id_max, lastdate = Add_new_articles_TheAtlantic(id_max, lastdate)\n",
        "        print(lastdate)\n",
        "    if found :\n",
        "        with open('presse_lastUpdate.csv', \"w\", newline='') as file:\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for row in readData:\n",
        "                if row['country'] == \"US\" and row['Publication'] == \"The Atlantic\" :\n",
        "                    row['maxId'] = id_max\n",
        "                    row['lastDate'] = lastdate\n",
        "                    break\n",
        "            writer.writerows(readData)\n",
        "    if not found:\n",
        "        with open('presse_lastUpdate.csv', \"a+\", newline='') as file:\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "            writer.writerow({'country' :'US', 'Publication' : 'The Atlantic',  'maxId' : id_max, 'lastDate' :lastdate})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "H3_j6-B4_AJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update():\n",
        "    #update_hespressFR()\n",
        "    #update_LeMatin()\n",
        "    #update_hespress()\n",
        "    updates_TheAtlantic()\n",
        "    #updates_ELMundo()\n",
        "update()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}